{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2:** The Hunt for the USS Scorpion (Part II)\n",
    "## Logistic regression, beta-binomial updating, and empirical Bayes\n",
    "STATS348, UChicago, Spring 2023\n",
    "\n",
    "----------------\n",
    "**Your name here:**\n",
    "Joonhyung Shin\n",
    "\n",
    "----------------\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/aschein/stat_348/blob/main/assignments/hw2/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "The purpose of this homework is to apply the concepts raised in [lecture 3](https://github.com/aschein/stat_348/blob/main/materials/3_logreg_and_beta_binomial.pdf):\n",
    "\n",
    "* logistic regression\n",
    "* overfitting\n",
    "* beta-Binomial conjugacy\n",
    "* empirical Bayes\n",
    "\n",
    "This homework will also familiarize you with the Python package [scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "Assignment is due **Monday April 3, 11:59pm** on GradeScope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "It is May 1968 and the USS _Scorpion_ has just disappeared somewhere in the Atlantic Ocean, likely off the coast of Spain. You are the lone statistician on board the USS _Mizar_, which has been dispatched to find the missing submarine. Your job is to guide the search as best you can, given the data at your disposal.\n",
    "\n",
    "## Search effectiveness probability (SEP)\n",
    "As we saw last week, an important component of our decision problem is the _search effectiveness probability_ of each search cell $k$\n",
    "\n",
    "\\begin{equation}\n",
    "q_k = P(\\textrm{finding the sub in $k$} \\mid \\textrm{sub is in $k$}, \\textrm{divers search $k$})\n",
    "\\end{equation}\n",
    "\n",
    "There are a number of factors that go into the SEP. The ocean floor is deeper in some cells, the ocean current is stronger, the water is murkier, etc. It's important we understand how easy it would be to actually detect the submarine in each cell before we send divers to look for it.\n",
    "\n",
    "In this assignment, we will explore two different ways of modeling $q_k$\n",
    "* Supervised learning, with logistic regression\n",
    "* Beta-binomial updating, and empirical Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# scientific computing libraries\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import scipy.stats as st\n",
    "import scipy.special as sp\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# progress bars for loops\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Learning SEPs with logistic regression\n",
    "\n",
    "## Setting\n",
    "\n",
    "In this part of the assignment, we will define SEPs across cells to be a function of covariates\n",
    "\n",
    "\\begin{equation}\n",
    "q_k \\triangleq q(\\boldsymbol{x}_k)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol{x}_k \\in \\mathbb{R}^p$ are covariates associated with each search cell that measure aspects which relate to divers' ability to find things (e.g., seafloor depth, strength of current, etc.), and perhaps polynomial expansions and interactions between those base covariates.\n",
    "\n",
    "In this section, we will work with **binary trial data**. In each of $K=100$ cells, we have dropped one large object and seen whether divers were able to recover it. Define the outcome to be\n",
    "\n",
    "\\begin{align}\n",
    "y_k &= \\begin{cases}\n",
    "1& \\textrm{ if divers recovered the object}\\\\\n",
    "0& \\textrm{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "We will work with **logistic regression** models that make the following assumption:\n",
    "\n",
    "\\begin{align}\n",
    "q_k \\equiv P(y_k = 1 \\mid \\boldsymbol{x}_k) = \\frac{\\exp(\\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k)}{1 + \\exp(\\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k)}\n",
    "\\end{align}\n",
    "\n",
    "Or, equivalently that\n",
    "\\begin{align}\n",
    "\\log (\\tfrac{q_k}{1-q_k}) = \\beta_0 + \\boldsymbol{\\beta}_1^\\top \\boldsymbol{x}_k\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the trial data\n",
    "Run the following cell, and explore the trial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>seafloor_depth</th>\n",
       "      <th>water_temperature</th>\n",
       "      <th>strength_of_current</th>\n",
       "      <th>seafloor_composition</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3581.873017</td>\n",
       "      <td>21.006783</td>\n",
       "      <td>moderate</td>\n",
       "      <td>sand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>715.353815</td>\n",
       "      <td>11.553155</td>\n",
       "      <td>strong</td>\n",
       "      <td>rock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2805.869109</td>\n",
       "      <td>10.707339</td>\n",
       "      <td>moderate</td>\n",
       "      <td>sand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1154.116879</td>\n",
       "      <td>16.352951</td>\n",
       "      <td>strong</td>\n",
       "      <td>mud</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1363.089831</td>\n",
       "      <td>12.815914</td>\n",
       "      <td>moderate</td>\n",
       "      <td>mud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>2659.833294</td>\n",
       "      <td>13.119218</td>\n",
       "      <td>strong</td>\n",
       "      <td>sand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>1032.545545</td>\n",
       "      <td>15.699824</td>\n",
       "      <td>weak</td>\n",
       "      <td>sand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>3883.862643</td>\n",
       "      <td>17.163361</td>\n",
       "      <td>moderate</td>\n",
       "      <td>mud</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>504.397526</td>\n",
       "      <td>10.698066</td>\n",
       "      <td>weak</td>\n",
       "      <td>rock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>2564.698389</td>\n",
       "      <td>18.361401</td>\n",
       "      <td>weak</td>\n",
       "      <td>sand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cell_id  seafloor_depth  water_temperature strength_of_current  \\\n",
       "0         0     3581.873017          21.006783            moderate   \n",
       "1         1      715.353815          11.553155              strong   \n",
       "2         2     2805.869109          10.707339            moderate   \n",
       "3         3     1154.116879          16.352951              strong   \n",
       "4         4     1363.089831          12.815914            moderate   \n",
       "..      ...             ...                ...                 ...   \n",
       "95       95     2659.833294          13.119218              strong   \n",
       "96       96     1032.545545          15.699824                weak   \n",
       "97       97     3883.862643          17.163361            moderate   \n",
       "98       98      504.397526          10.698066                weak   \n",
       "99       99     2564.698389          18.361401                weak   \n",
       "\n",
       "   seafloor_composition  outcome  \n",
       "0                  sand        1  \n",
       "1                  rock        1  \n",
       "2                  sand        1  \n",
       "3                   mud        0  \n",
       "4                   mud        1  \n",
       "..                  ...      ...  \n",
       "95                 sand        1  \n",
       "96                 sand        0  \n",
       "97                  mud        0  \n",
       "98                 rock        1  \n",
       "99                 sand        0  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logreg_trials = pd.read_csv('logreg_trial_cells.csv')\n",
    "df_logreg_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn how to preprocess features with `scikit-learn`\n",
    "\n",
    "Review the code in following cell carefully where we have provided code that takes a `Pandas` dataframe, and turns it into a `numpy` array corresponding to a scaled (samples $\\times$ features)-covariate matrix $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of covariate matrix X: (100, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def get_preprocessor(continuous_cols, categorical_cols, ordinal_cols, ordinal_categories):\n",
    "    \"\"\" Returns a preprocessor object that can be used in a scikit-learn Pipeline.\n",
    "\n",
    "    Args:\n",
    "        continuous_cols (list): list of column names for continuous features\n",
    "        categorical_cols (list): list of column names for categorical features\n",
    "        ordinal_cols (list): list of column names for ordinal features\n",
    "        ordinal_categories (list): list of lists of categories for ordinal features\n",
    "    \n",
    "    Returns:\n",
    "        preprocessor (ColumnTransformer): preprocessor object that can be used in a scikit-learn Pipeline\n",
    "    \"\"\"\n",
    "    continuous_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    ordinal_transformer = Pipeline(steps=[\n",
    "        ('ordinal', OrdinalEncoder(categories=ordinal_categories))\n",
    "    ])\n",
    "    \n",
    "    # Combine transformers into a single ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('continuous', continuous_transformer, continuous_cols),\n",
    "        ('categorical', categorical_transformer, categorical_cols),\n",
    "        ('ordinal', ordinal_transformer, ordinal_cols)\n",
    "    ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "continuous_cols = ['seafloor_depth', 'water_temperature']\n",
    "categorical_cols = ['seafloor_composition']\n",
    "ordinal_cols = ['strength_of_current']\n",
    "ordinal_categories = [['weak', 'moderate', 'strong']]\n",
    "\n",
    "preprocessor = get_preprocessor(continuous_cols, categorical_cols, ordinal_cols, ordinal_categories)\n",
    "\n",
    "X = preprocessor.fit_transform(df_logreg_trials)\n",
    "\n",
    "print(\"Shape of covariate matrix X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 [Code review, write] (5pts)\n",
    "1) Explain why the shape of the covariate matrix above is (100, 6). Specifically, what do the 6 features correspond to? \n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "The given code preprocesses the data to a numerical matrix by scaling continuous data and encoding categorical and ordinal data. A total of 4 columns were chosen as covariates, and `strength_of_current` column has 3 categories, `sand`, `rock`, and `mud`. Thus, one-hot encoding expands this to 3 columns, resulting in a total of 6 columns. The number of rows obviously remains unchanged.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more about the scikit-learn `Pipeline`\n",
    "\n",
    "The last cell used the scikit-learn `Pipeline` to define a preprocessor that takes in a dataframe and outputs covariate matrix. In the following cell, we provide code that creates a more sophisticated `Pipeline`, which further accepts a desired degree of polynomial expansion and again outputs a covariate matrix. Study the code carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def get_pipeline(preprocessor, poly_degree=1):\n",
    "    \"\"\" Returns a scikit-learn Pipeline object consisting of a preprocessor and polynomial feature transformer.\n",
    "    \n",
    "    Args:\n",
    "        preprocessor (ColumnTransformer): preprocessor object that can be used in a scikit-learn Pipeline\n",
    "        poly_degree (int): degree of polynomial features to generate\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('polynomial', PolynomialFeatures(degree=poly_degree, include_bias=True))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "pipeline = get_pipeline(preprocessor, poly_degree=2)\n",
    "X = pipeline.fit_transform(df_logreg_trials)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 [Code review, write] (10pts)\n",
    "1) Explain why the shape of the covariate matrix above is (100, 28). Specifically, what do the 28 features correspond to? In this case, do not list the name of each feature. Instead, provide a concise mathematical description of why there are 28 features.\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "The preprocessing class `PolynomialFeatures` generates additional features that are a polynomial of the existing covariates with degree up to `degree`. In our case, since `preprocessor` produces 6 covariates, `PolynomialFeatures` generates $6\\times 7\\div2=21$ second order terms, 6 first order terms, and 1 constant term (which is essentially an all-one column). Thus, the number of columns adds up to 28.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to use `LogisticRegression` in scikit-learn\n",
    "Study the following code carefully. At a high-level this code:\n",
    "\n",
    "* Preprocesses the data into two `numpy` arrays: $\\boldsymbol{y}, \\boldsymbol{X}$ \n",
    "* Defines a scikit-learn `LogisticRegression` model with no regularization\n",
    "* Fits the model to the data and computes the training **accuracy** and **negative log loss**\n",
    "* Defines a scikit-learn `KFold` object for cross-validation\n",
    "* Computes the average cross validation accuracy and negative log loss across the K folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/stat348/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.97\n",
      "Training negative log loss: -0.14\n",
      "Average k-fold accuracy: 0.49\n",
      "Average k-fold negative log loss: -3.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# binary outcomes\n",
    "y = df_logreg_trials['outcome'].values\n",
    "\n",
    "# degree-5 polynomial features\n",
    "degree = 5\n",
    "pipeline = get_pipeline(preprocessor, poly_degree=degree)\n",
    "X = pipeline.fit_transform(df_logreg_trials)\n",
    "\n",
    "# define logreg model with no regularization, saga solver, and 10,000 iterations\n",
    "logreg = LogisticRegression(penalty='none', solver='saga', max_iter=10000)\n",
    "\n",
    "# fit model to data\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# calculate accuracy on training data\n",
    "y_pred = logreg.predict(X)\n",
    "train_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "# calculate negative log loss on training data\n",
    "y_prob = logreg.predict_proba(X)\n",
    "train_neg_log_loss = -log_loss(y, y_prob)\n",
    "\n",
    "print('Training accuracy: {:.2f}'.format(train_accuracy))\n",
    "print('Training negative log loss: {:.2f}'.format(train_neg_log_loss))\n",
    "\n",
    "# define k-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# calculate average cross-validation accuracy across K folds\n",
    "avg_kfold_accuracy = cross_val_score(logreg, X, y, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
    "print('Average k-fold accuracy: {:.2f}'.format(avg_kfold_accuracy))\n",
    "\n",
    "# calculate average cross-validation negative log loss across K folds\n",
    "avg_kfold_neg_log_loss = cross_val_score(logreg, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1).mean()\n",
    "print('Average k-fold negative log loss: {:.2f}'.format(avg_kfold_neg_log_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 [Code review, write, math] (10pts)\n",
    "1) Explain why there might be a difference between the training versus cross validation accuracy (or negative log loss).\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "If the model overfits to the training data, the validation accuracy can be way lower than the training accuracy.\n",
    "\n",
    "___\n",
    "\n",
    "2) Write in mathematical form exactly what `-log_loss(y, y_prob)` computes. Your equation should be in terms of the SEPs $q_k$.\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "If $y\\in\\{0, 1\\}$ is the true label and $p$ is the predicted probability, the cross entropy loss is defined as\n",
    "\n",
    "$$\n",
    "    L_{\\text{log}}(y, p)=-(y\\log(p)+(1-y)\\log(1-p))\\,.\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this as\n",
    "\n",
    "$$\n",
    "    L_{\\text{log}}=\\begin{cases}\n",
    "        -\\log(p)&\\text{if $y=1$}\\\\\n",
    "        -\\log(1-p)&\\text{otherwise}\n",
    "    \\end{cases}\\,.\n",
    "$$\n",
    "\n",
    "Hence, in terms of $q_k$, `-log_loss(y, y_prob)` computes\n",
    "\n",
    "$$\n",
    "    \\sum_{k=1}^K(y_k\\log(q_k)+(1-y_k)\\log(1-q_k))=\\sum_{y_k=1}\\log(q_k)+\\sum_{y_k=0}\\log(1-q_k)\\,.\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 [Code] (10pts)\n",
    "\n",
    "Using what you have learned about how to preprocess data, fit a log-reg model, and use K-fold cross validation...\n",
    "\n",
    "1) Write code in the following cell that loops through polynomial degrees 1...5, and for each:\n",
    "    * Prepocesses the data using polynomial feature expansions of the given degree\n",
    "    * Fits a single unregularized logistic regression model\n",
    "    * Calculates and saves its training accuracy and negative log loss\n",
    "    * Creates a K-fold cross validation object\n",
    "    * Calculates and saves the average cross-validation accuracy and negative log loss across K folds\n",
    "\n",
    " \n",
    "2) Plot the results\n",
    "    * A plot depicting training versus average cross-validation **accuracy (y-axis)** as a function of **polynomial degree (x-axis)**\n",
    "    * A plot depicting training versus average cross-validation **negative log loss (y-axis)** as a function of **polynomial degree (x-axis)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5800e11b828344a2bd16fd2b3208d497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAFSCAYAAAD/8qVXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3FklEQVR4nO3de3xU1bn/8c+TgBCBqkdQA1EBGy7KYKIxKKASayv1WqkIWgvqUaSKWPxpqbW1VH/2p7UXe9FS9FTAerzDES3HC5UUpLVcNHKHWIo1JUVKFVETMeT5/bEnIYQEApmZPbPzfb9eec3svdfs/exMsuaZtddey9wdERERkSjJCjsAERERkURTgiMiIiKRowRHREREIkcJjoiIiESOEhwRERGJHCU4IiIiEjlKcEQkcsxsuJmtM7O3zezbTWw3M/tFfPtyMzspjDhFJHnahR3A/uratav37Nkz7DBEpJFly5b9y927hR2HmWUDDwBfBCqAJWY2x91XNyj2ZSA//jMI+HX8sVmqe0TSU3N1T8YlOD179mTp0qVhhyEijZjZO2HHEFcMvO3uGwDM7AngIqBhgnMRMNODkU5fN7NDzSzX3Sub26nqHpH01Fzdo0tUIhI1PYB3GyxXxNftbxkRyWBKcEQkaqyJdY3npGlJGcxsnJktNbOlW7ZsSUhwIpIaSnBEJGoqgKMbLOcBmw6gDO4+zd2L3L2oW7fQuxeJyH7IuD44Tfnss8+oqKiguro67FCklTp27EheXh7t27cPOxTJXEuAfDPrBfwDGA1c3qjMHGBCvH/OIGDb3vrfiEjmiUSCU1FRQZcuXejZsydmTbU8SyZwd7Zu3UpFRQW9evUKOxzJUO5eY2YTgJeAbOC37r7KzMbHt08F5gLnAm8DnwBXhRWviCRHJBKc6upqJTcRYGYcfvjhqK+DtJa7zyVIYhqum9rguQM3pDouEUmdyPTBUXITDXofRUQkESKT4IRp69atFBQUUFBQwFFHHUWPHj3ql3fs2LHX1y5dupSJEyfu8xiDBw9OVLgiIiKRF4lLVGE7/PDDKSsrA2DKlCl07tyZW265pX57TU0N7do1/asuKiqiqKhon8f405/+lJBYRVqkthbKy6GyEnJzIT8fsvR9SESSLIF1j2qsJLnyyiu5+eabKSkpYfLkySxevJjBgwdTWFjI4MGDWbduHQClpaWcf/75QJAcXX311QwbNozevXvzi1/8on5/nTt3ri8/bNgwLrnkEvr168fXvvY1gu4EMHfuXPr168fQoUOZOHFi/X5F9kttLcyaBYWFUFISPM6aFawXEUmWBNc9bbMFJ0XfTtevX8+8efPIzs7mww8/ZMGCBbRr14558+bxne98h2effXaP16xdu5b58+ezfft2+vbtyze+8Y09bpl+8803WbVqFd27d2fIkCEsWrSIoqIirrvuOhYsWECvXr247LLLEn4+0kaUl8OYMVBVFSxXVQXLsRj07RtubCKyuyi1tia47snQ30IrpPDb6ciRI8nOzgZg27ZtjBw5kgEDBjBp0iRWrVrV5GvOO+88OnToQNeuXTniiCPYvHnzHmWKi4vJy8sjKyuLgoICNm7cyNq1a+ndu3f97dVKcOSAVVbuqmDqVFUF60UkfUSttTXBdU/bS3CayxDLyxN+qE6dOtU//973vkdJSQkrV67k+eefb3ZQwg4dOtQ/z87OpqampkVl6i5TibRabi7k5Oy+LicnWC8i6SOFn2cpkeC6p+0lOCF9O922bRs9egRz+U2fPj3h++/Xrx8bNmxg48aNADz55JMJP4a0Efn5MHPmroomJydYzs8PNy4R2V3UWlsTXPe0vT44dRliwz+KFHw7/da3vsXYsWP56U9/yllnnZXw/efk5PDggw8yfPhwunbtSnFxccKPIW1EVhaMGBFc947CdX2RqArp8yxpElz3WKZd2igqKvKlS5futm7NmjX079+/ZTuou2ZZ16xXlyGOGJHxFfhHH31E586dcXduuOEG8vPzmTRpUthh7bf9ej8lbZjZMnff95gHGaqpukckVBH+PNsfzdU9ba8FJ8LfTh966CFmzJjBjh07KCws5Lrrrgs7JBERSZYIf54lQttLcCB48/v2jdwtr5MmTcrIFhsRETlAEf08SwSleSIiIhI5bbMFRyTRojTYlohIBKgGFmmtqA22JSISAUpwRForaoNtiYhEgBKcBPnnP//J6NGjOe644zj++OM599xzWb9+fdhh7eHKK6/kmWeeAeCaa65h9erVe5SZPn06EyZM2Ot+SktLd5vhfOrUqcycOTOxwWaKqA22JSISAeqDkwDuzsUXX8zYsWN54oknACgrK2Pz5s306dOnvtzOnTvr56ZKBw8//PABv7a0tJTOnTszePBgAMaPH5+osDJP1AbbEhGJALXgJMD8+fNp3779bh/yBQUFnH766ZSWllJSUsLll19OLBajurqaq666ilgsRmFhIfPnzwdg1apVFBcXU1BQwMCBAykvL+fjjz/mvPPO48QTT2TAgAF7TL+wZs2a3UYs3rhxIwMHDgTgzjvv5JRTTmHAgAGMGzeuybmqhg0bRt3AZY888gh9+vThzDPPZNGiRfVlnn/+eQYNGkRhYSFnn302mzdvZuPGjUydOpWf/exnFBQUsHDhQqZMmcKPf/xjIEjuTj31VAYOHMjFF1/M+++/X3+8yZMnU1xcTJ8+fVi4cGEifv3h09QGIiJpp00mOLW1sG4dlJYGj63tC7py5UpOPvnkZrcvXryYu+++m9WrV/PAAw8AsGLFCh5//HHGjh1LdXU1U6dO5aabbqKsrIylS5eSl5fHiy++SPfu3XnrrbdYuXIlw4cP322//fv3Z8eOHWzYsAEI5p+69NJLAZgwYQJLlixh5cqVVFVV8cILLzQbX2VlJd///vdZtGgRr7zyym6XrYYOHcrrr7/Om2++yejRo/nRj35Ez549GT9+PJMmTaKsrIzTTz99t/2NGTOGe++9l+XLlxOLxfjBD35Qv62mpobFixdz//3377Y+o9UNtvXmmzB/fvDYxkYSFckIia78Ja21uRo4jBteiouL6dWrFwCvvfYaX//614Fggsxjjz2W9evXc9ppp/HDH/6Qe++9l3feeYecnBxisRjz5s1j8uTJLFy4kEMOOWSPfV966aU89dRTQJDgjBo1CghalQYNGkQsFuPVV19l1apVzcb3l7/8hWHDhtGtWzcOOuig+n0AVFRUcM455xCLxbjvvvv2uh8IJhX94IMPOPPMMwEYO3YsCxYsqN8+YsQIAE4++eT6iUEjoW6wrWHDgkclNyLpRXc7tjltrhZOxg0vJ5xwAsuWLWt2e6dOneqfNzf31+WXX86cOXPIycnhnHPO4dVXX6VPnz4sW7aMWCzGbbfdxp133rnH60aNGsVTTz3F+vXrMTPy8/Oprq7m+uuv55lnnmHFihVce+21VFdX7/UczKzJ9TfeeCMTJkxgxYoV/OY3v9nnfvalQ4cOAGRnZ1NTU9OqfYmItJjudmxz2lyCk4wbXs466yw+/fRTHnroofp1S5Ys4Y9//OMeZc844wwee+wxANavX8/f//53+vbty4YNG+jduzcTJ07kwgsvZPny5WzatImDDz6YK664gltuuYU33nhjj/0dd9xxZGdnc9ddd9W3vNQlIV27duWjjz6qv2uqOYMGDaK0tJStW7fy2Wef8fTTT9dv27ZtGz169ABgxowZ9eu7dOnC9u3b99jXIYccwmGHHVbfv+bRRx+tb80REQmN7nZsc9rcXVTJuOHFzJg9ezbf/OY3ueeee+jYsSM9e/bk/vvv5x//+MduZa+//nrGjx9PLBajXbt2TJ8+nQ4dOvDkk0/yu9/9jvbt23PUUUdxxx13sGTJEm699VaysrJo3749v/71r5s8/qhRo7j11lv529/+BsChhx7KtddeSywWo2fPnpxyyin7+J3kMmXKFE477TRyc3M56aST2LlzJwBTpkxh5MiR9OjRg1NPPbX+GBdccAGXXHIJzz33HL/85S9329+MGTMYP348n3zyCb179+aRRx45oN+riEjC6G7HNseau2SSroqKirzuzp86a9asoX///i16vWaXT3/7835K+jCzZe5eFHYcydJU3SMZRJV/ZDVX97S5FhzNLi8i0gap8m9z2lyCA5pdXkSkTVLl36YodRUREZHIiUwLjrs3e6uzZI5M6xMm6cXM/gN4EugJbAQudff3myi3EdgO7ARqotx3SKStikQLTseOHdm6das+HDOcu7N161Y6duwYdiiSub4N/MHd84E/xJebU+LuBUpuRKIpEi04eXl5VFRUsGXLlrBDkVbq2LEjeXl5YYchmesiYFj8+QygFJgcVjAiEp5IJDjt27evnwpBRNq0I929EsDdK83siGbKOfCymTnwG3eflrIIRSQlIpHgiEjbYWbzgKOa2HT7fuxmiLtviidAr5jZWndf0LiQmY0DxgEcc8wxBxSviIRDCY6IZBR3P7u5bWa22cxy4603ucB7zexjU/zxPTObDRQDeyQ48ZadaRAM9JeI+EUkNSLRyVhEJG4OMDb+fCzwXOMCZtbJzLrUPQe+BKxMWYSZpLYW1q2D0tLgUTNvSwZRgiMiUXIP8EUzKwe+GF/GzLqb2dx4mSOB18zsLWAx8Ht3fzGUaNNZ3dQGhYVQUhI8zpqlJEcyhi5RiUhkuPtW4AtNrN8EnBt/vgE4McWhZZ7y8l3zNkHwOGZMMNWBRgKWDKAWHBER2VNl5e4zb0OwXFkZTjwi+ympCY6ZDTezdWb2tpntMeCWmR1mZrPNbLmZLTazAcmMR0REWig3N5hxu6GcnGC9SAZIWoJjZtnAA8CXgeOBy8zs+EbFvgOUuftAYAzw82TFIyKSVFHrkJufDzNn7kpycnKC5fz8cOMSaaFk9sEpBt6OX+/GzJ4gGGV0dYMyxwP/D8Dd15pZTzM70t03JzEuEZHEquuQW9dnpS4ZGDEimME6E2VlBfHHYsFlqdzcILnJ1PORNieZf6k9gHcbLFfE1zX0FjACwMyKgWMBjdMvIpmluQ655eXhxtVaWVlBh+Jhw4JHJTeSQZL519rU1N6NB8q6BzjMzMqAG4E3gZo9dmQ2zsyWmtlSzTclImlHHXJF0k4yL1FVAEc3WM4DNjUs4O4fAlcBmJkBf4v/0KicRhMVkfSVm0vt5/Mpv/R2Kq0HuWwi/8n/S5Y65IqEJpkJzhIg38x6Af8ARgOXNyxgZocCn7j7DuAaYEE86RERyRi1x+Uz6/ZljLm+y64uOA9ezIjjOmksDpGQJO1/z91rgAnAS8Aa4Cl3X2Vm481sfLxYf2CVma0luNvqpmTFIyKSLOV/zapPbiDeBef6LpT/VemNSFiSOpKxu88F5jZaN7XB8z8DuudQRDLa3rrgaNBfkXDo64WISCtpTDyR9KMER0SklTQmnkj60WSbIiKtpDHxRNKPEhwRkQSoGxNPfW5E0oO+X4iIiEjkKMERERGRyFGCIyIiIpGjPjgiItKk2tpgvlB1nJZMpD9VERHZQ20tzJoFhYVQUhI8zpoVrBfJBEpwRERkD+XlMGYMu08/MSZYL5IJlOCIiMge9jb9hEgmUIIjIiJ70PQTkumU4IiIyB40/YRkOt1FJSIie9D0E5LplOCIiEiTNP2EZDLl4iIiIhI5SnBEREQkcpTgiIiISOQowREREZHIUYIjIpFhZiPNbJWZ1ZpZ0V7KDTezdWb2tpl9O5UxikhqKMERkShZCYwAFjRXwMyygQeALwPHA5eZ2fGpCU9EUkW3iYtIZLj7GgAz21uxYuBtd98QL/sEcBGwOukBikjKqAVHRNqaHsC7DZYr4utEJELUgiMiGcXM5gFHNbHpdnd/riW7aGKdN3OsccA4gGOOOabFMYpI+JTgiEhGcfezW7mLCuDoBst5wKZmjjUNmAZQVFTUZBIkIulJl6hEpK1ZAuSbWS8zOwgYDcwJOSYRSTAlOCISGWZ2sZlVAKcBvzezl+Lru5vZXAB3rwEmAC8Ba4Cn3H1VWDGLSHLoEpWIRIa7zwZmN7F+E3Bug+W5wNwUhiYiKaYWHBEREYkcJTgiIiISOUpwREREJHKU4IiIiEjkKMERERGRyFGCIyIiIpGjBEdEREQiRwmOiIiIRI4SHBEREYkcJTgiIiISOUpwREREJHI0F5WEo7YWysuhshJycyE/H7KUb4uISGLoE0VSr7YWZs2CwkIoKQkeZ80K1ouIiCSAEhxJvfJyGDMGqqqC5aqqYLm8PNy4REQkMpTgSOpVVu5KbupUVQXrRUREEiCpCY6ZDTezdWb2tpl9u4nth5jZ82b2lpmtMrOrkhmPpIncXMjJ2X1dTk6wXkREJAGSluCYWTbwAPBl4HjgMjM7vlGxG4DV7n4iMAz4iZkdlKyYJE3k58PMmbuSnJycYDk/P9y4REQkMpJ5F1Ux8La7bwAwsyeAi4DVDco40MXMDOgM/BuoSWJMkg6ysmDECIjFdBeViIgkRTITnB7Auw2WK4BBjcr8CpgDbAK6AKPcXbfStAVZWdC3b/AjIiKSYMn8ymxNrPNGy+cAZUB3oAD4lZl9bo8dmY0zs6VmtnTLli2JjlNEREQiJpkJTgVwdIPlPIKWmoauAmZ54G3gb0C/xjty92nuXuTuRd26dUtawCIiIhINyUxwlgD5ZtYr3nF4NMHlqIb+DnwBwMyOBPoCG5IYk4iIiLQBSeuD4+41ZjYBeAnIBn7r7qvMbHx8+1TgLmC6ma0guKQ12d3/layYREREpG1I6lxU7j4XmNto3dQGzzcBX0pmDCIiItL26L5cERERiRwlOCIiIhI5SnBEREQkcpTgiIiISOQowRGRyDCzkfGJe2vNrGgv5Taa2QozKzOzpamMUURSI6l3UYmIpNhKYATwmxaULdGwFCLRpQRHRCLD3dcABPP3ikhbpktUIhIKM7vJzD5ngf8yszfMLFXjYjnwspktM7NxKTqmiKSQEhwRCcvV7v4hwWCf3QjmprtnXy8ys3lmtrKJn4v249hD3P0k4MvADWZ2RjPH0kS/IhlKl6hEJCx115HOBR5x97esBdeW3P3s1h44Poo67v6emc0GioEFTZSbBkwDKCoq8tYeV0RSRy04IhKWZWb2MkGC85KZdQFqk31QM+sUPxZm1omgBWllso8rIqmlBEdEwvKfwLeBU9z9E6A9wWWqA2ZmF5tZBXAa8Hszeym+vruZ1c2LdyTwmpm9BSwGfu/uL7bmuCKSfnSJSkTCchpQ5u4fm9kVwEnAz1uzQ3efDcxuYv0mgpYi3H0DcGJrjiMi6U8tOCISll8Dn5jZicC3gHeAmeGGJCJRoQRHRMJS4+4OXAT83N1/DnQJOSYRiQhdohKRsGw3s9uArwOnm1k2QT8cEZFWUwuOiIRlFPApwXg4/wR6APeFG5KIRIUSHBEJRTypeQw4xMzOB6rdXX1wRCQh9pngmNn5ZqZESEQSyswuJbhNeyRwKfAXM7sk3KhEJCpa0gdnNPBzM3uWYLTRNUmOSUTahtsJxsB5D8DMugHzgGdCjUpEImGfLTPufgVQCPwVeMTM/hyfn0V3O4hIa2TVJTdxW9FlcxFJkBZVJvEJ8Z4FngBygYuBN8zsxiTGJiLR9qKZvWRmV5rZlcDvgbn7eI2ISIvs8xKVmV0AXA0cBzwKFMcnqDsYWAP8MrkhikgUufutZvZVYAjBxJvT4iMRi4i0Wkv64IwEfubuu8206+6fmNnVyQlLRNoCd3+WoHVYRCShWpLgfB+orFswsxzgSHff6O5/SFpkIhJJZrYd8KY2Ae7un0txSCISQS1JcJ4GBjdY3hlfd0pSIhKRSHN33aAgIknXkk7G7dx9R91C/PlByQtJREREpHVakuBsMbML6xbM7CLgX8kLSURERKR1WnKJajzwmJn9iuAa+bvAmKRGJSIiItIK+0xw3P2vwKlm1hkwd9+e/LBEpC0ws2OBfHefF7+BoZ3qGBFJhJa04GBm5wEnAB3NDAB3vzOJcYlIxJnZtcA44D8IxtnKA6YCXwgzLhGJhpZMtjkVGAXcSHCJaiRwbJLjEpHou4FgkL8PAdy9HDgi1IhEJDJa0sl4sLuPAd539x8ApwFHJzcsEWkDPm14h6aZtaPp8XFERPZbSxKc6vjjJ2bWHfgM6JW8kESkjfijmX0HyDGzLxKMr/V8yDGJSES0JMF53swOBe4D3gA2Ao8nMSYRaRu+DWwBVgDXEUy0+d1QIxKRyNhrJ2MzywL+4O4fAM+a2QtAR3fflorgRCTSLgJmuvtDYQciItGz1xYcd68FftJg+VMlNyKSIBcC683sUTM7L94HR0QkIVpyieplM/uq1d0fLiKSAO5+FfB5gr43lwN/NbOHw41KRKKiJd+YbgY6ATVmVo1m/BWRBHH3z8zsfwnunsohuGx1TbhRiUgUtGQkY838KyIJZ2bDgdFACVAKPAxcGmZMIhId+0xwzOyMpta7+4LEhyMibciVwBPAde7+acixiEjEtOQS1a0NnncEioFlwFlJiUhE2gR3H53ofZrZfcAFwA7gr8BV8btAG5cbDvwcyAYedvd7Eh2LiIRrn52M3f2CBj9fBAYAm5MfmohEkZm9Fn/cbmYfNvjZbmYftnL3rwAD3H0gsB64rYnjZwMPAF8GjgcuM7PjW3lcEUkzLbmLqrEKgiRnn8xsuJmtM7O3zezbTWy/1czK4j8rzWynmf3HAcQkIhnC3YfGH7u4++ca/HRp7c0L7v6yu9fEF18nmMCzsWLgbXffEJ8q4gmCzs0iEiEt6YPzS3bND5MFFABvteB1dd+SvkiQFC0xsznuvrqujLvfRzBCMmZ2ATDJ3f+9n+cQfbW1UF4OlZWQmwv5+ZB1ILmpSPows0fd/ev7WtcKVwNPNrG+B/Bug+UKYFCCjikiaaIlfXCWNnheAzzu7ota8Lr6b0kAZlb3LWl1M+UvQ1NA7Km2FmbNgjFjoKoKcnJg5kwYMUJJjmS6ExouxAf6O3lfLzKzecBRTWy63d2fi5e5naC+eqypXTSxrslJPs1sHDAO4JhjjtlXaCKSRlqS4DwDVLv7TghaZszsYHf/ZB+va/G3JDM7GBgOTGhBPG1Lefmu5AaCxzFjIBaDvn3DjU3kAJjZbUDdJJt1fW6MoGPwtH293t3P3sf+xwLnA19w96YSlwrg6AbLecCmZo41rS6moqIizXQukkFa0gTwB4IBuOrkAPNa8LoWf0siuOthUXOXp8xsnJktNbOlW7ZsacGhI6SycldyU6eqKlgvkoHc/f/Fx9e6r1H/m8PdfY9OwfsjfnfUZODCvXwJWwLkm1kvMzuIYCyeOa05roikn5YkOB3d/aO6hfjzg1vwuhZ/SyKoYJq9POXu09y9yN2LunXr1oJDR0hubnBZqqGcnGC9SAZz99vM7DAzKzazM+p+WrnbXwFdgFfiNy9MBTCz7mY2N37cGoLW4peANcBT7r6qlccVkTTTkktUH5vZSe7+BoCZnQxU7eM10OBbEvAPgiTm8saFzOwQ4EzgihZH3Zbk5wd9bhr3wcnPDzsykVYxs2uAmwi+/JQBpwJ/phVjbLn755tZvwk4t8HyXGDugR5HRNJfSxKcbwJPm1ld60suMGpfL3L3GjOr+5aUDfzW3VeZ2fj49qnxohcDL7v7x/sbfJuQlRV0KI7FdBeVRM1NwCnA6+5eYmb9gB+EHJOIRERL5qJaEq94+hL0q1nr7p+1ZOdNfUtqkNjULU8Hprcw3rYpKyvoUKxOxRIt1e5ebWaYWQd3X2tm+iMXkYTYZzOAmd0AdHL3le6+AuhsZtcnPzQRibgKMzsU+B+CPjPP0Xw/PRGR/dKS6xzXNpzLxd3fB65NWkQi0ia4+8Xu/oG7TwG+B/wX8JVQgxKRyGhJH5wsM7O68STiIxQflNywRCTqGk3LsiL+qLFmRCQhWpLgvAQ8Fb/d0oHxwP8mNSoRaQveIBhK4n2C/n2HApVm9h5By/GyEGMTkQzXkktUkwkG+/sGcAOwnN0H/hMRORAvAue6e1d3P5xgdu+ngOuBB0ONTEQy3j4THHevJZiVdwNQBHyBYHAsEZHWKHL3l+oW3P1l4Ax3fx3oEF5YIhIFzV6iMrM+BIPzXQZsJT4rr7uXpCY0EYm4f5vZZOCJ+PIo4P14P7/a8MISkSjYWwvOWoLWmgvcfai7/xLYmZqwRKQNuJxgFOP/if8cHV+XDVwaWlQiEgl762T8VYIWnPlm9iLBt6ymJtAUEdlv7v4v4EYz69xwvru4t8OISUSio9kWHHef7e6jgH5AKTAJONLMfm1mX0pRfCISUWY22MxWA6vjyyeamToXi0hCtKST8cfu/pi7n8+uSfG+nezARCTyfgacQ9DHD3d/C2jtbOIiIkDLbhOv5+7/dvffuPsBz/YrIlLH3d9ttEr9/EQkIVoy0J+ISDK8a2aDATezg4CJaAgKEUmQ/WrBERFJoPEEg4f2ACqAgviyiEirqQVHREIRv4vqa2HHISLRpARHRFLKzO7Yy2Z397tSFoyIRJYSHBFJtY+bWNcJ+E/gcEAJjoi0mhIcEUkpd/9J3XMz6wLcBFxFMJjoT5p7nYjI/lCCIyIpZ2b/AdxM0AdnBnCSu78fblQiEiVKcEQkpczsPmAEMA2INTFNg4hIq+k2cRFJtf8DdAe+C2wysw/jP9vN7MOQYxORiFALjoiklLvri5WIJJ0qGhEREYkcJTgiIiISOUpwREREJHKU4IiIiEjkqJOxiERG/Bb0C4AdwF+Bq9z9gybKbQS2AzuBGncvSmGYIpICasERkSh5BRjg7gOB9cBteylb4u4FSm5EokkJjohEhru/7O418cXXgbww4xGR8CjBEZGouhr432a2OfCymS0zs3EpjElEUkR9cEQko5jZPOCoJjbd7u7PxcvcDtQAjzWzmyHuvsnMjgBeMbO17r6giWONA8YBHHPMMQmJX0RSQwmOiGQUdz97b9vNbCxwPvAFd/dm9rEp/viemc0GioE9Ehx3n0YwZxZFRUVN7ktE0pMuUYlIZJjZcGAycKG7f9JMmU5m1qXuOfAlYGXqohSRVIhmglNbC+vWQWlp8FhbG3ZEIpIavwK6EFx2KjOzqQBm1t3M5sbLHAm8ZmZvAYuB37v7i+GEKyLJEr1LVLW1MGsWjBkDVVWQkwMzZ8KIEZAVzXxORALu/vlm1m8Czo0/3wCcmMq4RCT1oveJX16+K7mB4HHMmGC9iIiItAnRS3AqK3clN3WqqoL1IiIi0iZE7xJVbm5wWaphkpOTE6wXEZE2q7Y2aMyvrAw+EvLz1XMh3STyPYreW5ufH/S5yckJluv64OTnhxuXiIiEpq57ZmEhlJQEj7Nm6R6UdJLo9yh6LThZWUGH4lhMabqIiADNd8+MxaBv33Bjk0Ci36NofupnZQW/jWHDgkclNyIibZq6Z6a/RL9H+uQXEZHIq+ue2ZC6Z6aXRL9HSnBERCTy1D0z/SX6PUpqH5z4sOk/B7KBh939nibKDAPuB9oD/3L3M5MZk4iItD3qnpn+Ev0eJS3BMbNs4AHgi0AFsMTM5rj76gZlDgUeBIa7+9/jM/uKiIgkXF33THUqTl+JfI+SmbsWA2+7+wZ33wE8AVzUqMzlwCx3/zsEM/smMR4RERFpI5KZ4PQA3m2wXBFf11Af4DAzKzWzZWY2JonxiIiISBuRzD441sQ6b+L4JwNfAHKAP5vZ6+6+frcdmY0DxgEcc8wxSQhVREREoiSZLTgVwNENlvOATU2UedHdP3b3fwELaGKWX3ef5u5F7l7UrVu3pAUsIiIi0ZDMBGcJkG9mvczsIGA0MKdRmeeA082snZkdDAwC1iQxJhEREWkDknaJyt1rzGwC8BLBbeK/dfdVZjY+vn2qu68xsxeB5UAtwa3kK5MVk4iIiLQNSR0Hx93nAnMbrZvaaPk+4L5kxiEiIiJti4Y4EhERkchRgiMiIiKRowRHREREIkcJjoiIiESOEhwRERGJHCU4IiIiEjlKcERERCRykjoOTlhqa6G8HCorITcX8vODKdglfeg9EhGRZIrcR0ptLcyaBYWFUFISPM6aFayX9KD3SEREki1yCU55OYwZA1VVwXJVVbBcXh5uXLKL3iMREUm2yCU4lZW7PjjrVFUF6yU96D0SEZFki1yCk5sLOTm7r8vJCdZLetB7JCIiyRa5BCc/H2bO3PUBmpMTLOfnhxuX7KL3SJLFzO4ys+VmVmZmL5tZ92bKDTezdWb2tpl9O9VxikjyRe4uqqwsGDECYjHdoZOu9B5JEt3n7t8DMLOJwB3A+IYFzCwbeAD4IlABLDGzOe6+OtXBikjyRC7BgeCDsm/f4EfSk94jSQZ3/7DBYifAmyhWDLzt7hsAzOwJ4CJACY5IhEQywRGRtsvM7gbGANuAkiaK9ADebbBcAQxqZl/jgHEAxxxzTGIDFZGk0kUBEckoZjbPzFY28XMRgLvf7u5HA48BE5raRRPrmmrpwd2nuXuRuxd169YtcSchIkmnFhwRySjufnYLi/438Hvg+43WVwBHN1jOAzYlIDQRSSNqwRGRyDCzhvfiXQisbaLYEiDfzHqZ2UHAaGBOKuITkdRRC46IRMk9ZtYXqAXeIX4HVfx28Yfd/Vx3rzGzCcBLQDbwW3dfFVrEIpIUSnBEJDLc/avNrN8EnNtgeS4wN1VxiUjq6RKViIiIRI4SHBEREYkcJTgiIiISOUpwREREJHKU4IiIiEjkKMERERGRyFGCIyIiIpGjBEdEREQiRwmOiIiIRI4SHBEREYkcJTgiIiISOUpwREREJHKU4IiIiEjkKMERERGRyFGCIyIiIpGjBEdEREQiRwmOiIiIRI4SHBEREYmcdmEHIPtWWwvl5VBZCbm5kJ8PWUpNRUREmqWPyTRXWwuzZkFhIZSUBI+zZgXrRUREpGlKcNJceTmMGQNVVcFyVVWwXF4eblwiIiLpTAlOmqus3JXc1KmqCtaLiIhI05LaB8fMhgM/B7KBh939nkbbhwHPAX+Lr5rl7ncmM6ZMk5sLOTm7Jzk5OcF6EQnXZ599RkVFBdXV1WGHIq3UsWNH8vLyaN++fdihSIIkLcExs2zgAeCLQAWwxMzmuPvqRkUXuvv5yYoj0+Xnw8yZuy5T5eQEy/n5YUcmIhUVFXTp0oWePXtiZmGHIwfI3dm6dSsVFRX06tUr7HAkQZLZglMMvO3uGwDM7AngIqBxgiN7kZUFI0ZALKa7qETSTXV1tZKbCDAzDj/8cLZs2RJ2KJJAyUxwegDvNliuAAY1Ue40M3sL2ATc4u6rkhhTRsrKgr59gx8RSS9KbqJB72P0JLMdoKm/Fm+0/AZwrLufCPwS+J8md2Q2zsyWmtlSZdgiIoGtW7dSUFBAQUEBRx11FD169Khf3rFjx15fu3TpUiZOnLjPYwwePDhR4YqkVDJbcCqAoxss5xG00tRz9w8bPJ9rZg+aWVd3/1ejctOAaQBFRUWNkyQRkTbp8MMPp6ysDIApU6bQuXNnbrnllvrtNTU1tGvXdDVfVFREUVHRPo/xpz/9KSGxiqRaMltwlgD5ZtbLzA4CRgNzGhYws6Ms3i5oZsXxeLYmMSYRiTAzu8vMlptZmZm9bGbdmym30cxWxMstTVmAtbWwbh2UlgaPSRix88orr+Tmm2+mpKSEyZMns3jxYgYPHkxhYSGDBw9m3bp1AJSWlnL++cH9HVOmTOHqq69m2LBh9O7dm1/84hf1++vcuXN9+WHDhnHJJZfQr18/vva1r+EefN+cO3cu/fr1Y+jQoUycOLF+vyJhSloLjrvXmNkE4CWC28R/6+6rzGx8fPtU4BLgG2ZWA1QBo73uP0ZEZP/d5+7fAzCzicAdwPhmypY0bi1OqrphyRvfEjliRMLvGli/fj3z5s0jOzubDz/8kAULFtCuXTvmzZvHd77zHZ599tk9XrN27Vrmz5/P9u3b6du3L9/4xjf2uGX6zTffZNWqVXTv3p0hQ4awaNEiioqKuO6661iwYAG9evXisssuS+i5iByopI6D4+5zgbmN1k1t8PxXwK+SGYOItB0NL3sDndiz3194mhuWPBZL+B0EI0eOJDs7G4Bt27YxduxYysvLMTM+++yzJl9z3nnn0aFDBzp06MARRxzB5s2bycvL261McXFx/bqCggI2btxI586d6d27d/3t1ZdddhnTpk1L6PmIHAjdbCwikWJmd5vZu8DXCFpwmuLAy2a2zMzGpSSwFA5L3qlTp/rn3/ve9ygpKWHlypU8//zzzQ5K2KFDh/rn2dnZ1NTUtKiMGt0lXSnBEZGMYmbzzGxlEz8XAbj77e5+NPAYMKGZ3Qxx95OALwM3mNkZzRwrcXdw1g1L3lAKhiXftm0bPXr0AGD69OkJ33+/fv3YsGEDGzduBODJJ59M+DFEDoQSHBHJKO5+trsPaOLnuUZF/xv4ajP72BR/fA+YTTAwaVPlprl7kbsXdevWrXWB1w1LXpfkpGhY8m9961vcdtttDBkyhJ07dyZ8/zk5OTz44IMMHz6coUOHcuSRR3LIIYck/Dgi+8syrXmxqKjIly5N3U0PItIyZrbM3fd933FyY8h39/L48xuBM939kkZlOgFZ7r49/vwV4E53f3Fv+26q7lmzZg39+/dveYC1tUFfnIgNS/7RRx/RuXNn3J0bbriB/Px8Jk2aFHZY+22/309JC83VPUntZCwikmL3mFlfoBZ4h/gdVPHbxR9293OBI4HZ8REq2gH/va/kJmEiOiz5Qw89xIwZM9ixYweFhYVcd911YYckogRHRKLD3fd2Serc+PMNwImpjCvqJk2alJEtNhJtmd82KiIiItKIEhwRERGJHCU4IiIiEjlKcERERCRylOCIiGSwf/7zn4wePZrjjjuO448/nnPPPZf169eHHdYerrzySp555hkArrnmGlavXr1HmenTpzNhQnNjMwZKS0t3m+F86tSpzJw5M7HBSiToLioRkQzl7lx88cWMHTuWJ554AoCysjI2b95Mnz596svt3Lmzfm6qdPDwww8f8GtLS0vp3LkzgwcPBmD8+ObmUpW2Ti04IiIpUlsL69ZBaWnwWFvbuv3Nnz+f9u3b7/YhX1BQwOmnn05paSklJSVcfvnlxGIxqqurueqqq4jFYhQWFjJ//nwAVq1aRXFxMQUFBQwcOJDy8nI+/vhjzjvvPE488UQGDBiwx/QLa9asobh41+DPGzduZODAgQDceeednHLKKQwYMIBx48Y1OVfVsGHDqBs08ZFHHqFPnz6ceeaZLFq0qL7M888/z6BBgygsLOTss89m8+bNbNy4kalTp/Kzn/2MgoICFi5cyJQpU/jxj38MBMndqaeeysCBA7n44ot5//336483efJkiouL6dOnDwsXLmzdL14yghIckQRI9AdX2KJ2PumgthZmzYLCQigpCR5nzWrd73blypWcfPLJzW5fvHgxd999N6tXr+aBBx4AYMWKFTz++OOMHTuW6upqpk6dyk033URZWRlLly4lLy+PF198ke7du/PWW2+xcuVKhg8fvtt++/fvz44dO9iwYQMQzD916aWXAjBhwgSWLFnCypUrqaqq4oUXXmg2vsrKSr7//e+zaNEiXnnlld0uWw0dOpTXX3+dN998k9GjR/OjH/2Inj17Mn78eCZNmkRZWRmnn376bvsbM2YM9957L8uXLycWi/GDH/ygfltNTQ2LFy/m/vvv3219ptP/avOU4Ii0UjI+uMIUtfNJF+XlMGbMrgnFq6qC5fLy5B2zuLiYXr16AfDaa6/x9a9/HQgmyDz22GNZv349p512Gj/84Q+59957eeedd8jJySEWizFv3jwmT57MwoULm5xb6tJLL+Wpp54CggRn1KhRQNCqNGjQIGKxGK+++iqrVq1qNr6//OUvDBs2jG7dunHQQQfV7wOgoqKCc845h1gsxn333bfX/UAwqegHH3zAmWeeCcDYsWNZsGBB/fYRI0YAcPLJJ9dPDJrp9L+6d0pwRFopjA+uZIra+aSLyspdv9M6VVXB+gN1wgknsGzZsma3d+rUqf55c/MOXn755cyZM4ecnBzOOeccXn31Vfr06cOyZcuIxWLcdttt3HnnnXu8btSoUTz11FOsX78eMyM/P5/q6mquv/56nnnmGVasWMG1115LdXX1Xs8hPmXGHm688UYmTJjAihUr+M1vfrPP/exLhw4dAMjOzqampqZV+0oX+l/dOyU4Iq2UjA+uMEXtfNJFbu6uicTr5OQE6w/UWWedxaeffspDDz1Uv27JkiX88Y9/3KPsGWecwWOPPQbA+vXr+fvf/07fvn3ZsGEDvXv3ZuLEiVx44YUsX76cTZs2cfDBB3PFFVdwyy238MYbb+yxv+OOO47s7Gzuuuuu+paXuiSka9eufPTRR/V3TTVn0KBBlJaWsnXrVj777DOefvrp+m3btm2jR48eAMyYMaN+fZcuXdi+ffse+zrkkEM47LDD6vvXPProo/WtOVGl/9W9U4Ij0krJ+OAKU9TOJ13k58PMmbt+tzk5wXJ+/oHv08yYPXs2r7zyCscddxwnnHACU6ZMoXv37nuUvf7669m5cyexWIxRo0Yxffp0OnTowJNPPsmAAQMoKChg7dq1jBkzhhUrVtR3PL777rv57ne/2+TxR40axe9+97v6/jeHHnoo1157LbFYjK985Succsope40/NzeXKVOmcNppp3H22Wdz0kkn1W+bMmUKI0eO5PTTT6dr16716y+44AJmz55d38m4oRkzZnDrrbcycOBAysrKuOOOO1r8u8xE+l/dO2uu2TJdFRUVeV3ve5F0UHcdvK6puO6Da8SIYPLoTHOg52Nmy9y9KHWRplZTdc+aNWvo379/i/dRWxtcPqisDD6E8vMz828kqvb3/Qxb1OqeA9Vc3aNxcERaKSsrqFBisWh8cEXtfNJJVhb07Rv8iLSW/lf3TgmOSAJE7YMraucjElX6X22e8jwRERGJHCU4IiKtkGn9GKVpeh+jRwmOiMgB6tixI1u3btWHY4Zzd7Zu3UrHjh3DDkUSSH1wREQOUF5eHhUVFWzZsiXsUKSVOnbsSF5eXthhSAIpwREROUDt27evnwpBRNKLLlGJiIhI5CjBERERkchRgiMiIiKRk3FTNZjZFuCdFhbvCvwrieGkWtTOB6J3Tm35fI51927JDCZM+1H3RO1vAKJ3TlE7H4jeObW67sm4BGd/mNnSKM2NE7Xzgeidk85Hovg7i9o5Re18IHrnlIjz0SUqERERiRwlOCIiIhI5UU9wpoUdQIJF7Xwgeuek85Eo/s6idk5ROx+I3jm1+nwi3QdHRERE2qaot+CIiIhIGxTJBMfMfmtm75nZyrBjSQQzO9rM5pvZGjNbZWY3hR1Ta5hZRzNbbGZvxc/nB2HHlAhmlm1mb5rZC2HHkghmttHMVphZmZktDTuedKd6J/2p7skMiap7InmJyszOAD4CZrr7gLDjaS0zywVy3f0NM+sCLAO+4u6rQw7tgJiZAZ3c/SMzaw+8Btzk7q+HHFqrmNnNQBHwOXc/P+x4WsvMNgJF7h6lsTWSRvVO+lPdkxkSVfdEsgXH3RcA/w47jkRx90p3fyP+fDuwBugRblQHzgMfxRfbx38yOtM2szzgPODhsGORcKjeSX+qe9qWSCY4UWZmPYFC4C8hh9Iq8SbVMuA94BV3z+jzAe4HvgXUhhxHIjnwspktM7NxYQcj4YlKvQOqezJEQuoeJTgZxMw6A88C33T3D8OOpzXcfae7FwB5QLGZZWyTvpmdD7zn7svCjiXBhrj7ScCXgRvil2CkjYlSvQOqezJEQuoeJTgZIn69+FngMXefFXY8ieLuHwClwPBwI2mVIcCF8evGTwBnmdnvwg2p9dx9U/zxPWA2UBxuRJJqUa13QHVPOktU3aMEJwPEO8b9F7DG3X8adjytZWbdzOzQ+PMc4GxgbahBtYK73+buee7eExgNvOruV4QcVquYWad4x1LMrBPwJSASdwdJy0St3gHVPZkgkXVPJBMcM3sc+DPQ18wqzOw/w46plYYAXyfIzsviP+eGHVQr5ALzzWw5sITgOngkbm+MkCOB18zsLWAx8Ht3fzHkmNKa6p2MoLon/SWs7onkbeIiIiLStkWyBUdERETaNiU4IiIiEjlKcERERCRylOCIiIhI5CjBERERkchpF3YAkpnMbCewgmAulxpgBnC/u0dpuHARSTOqe6SllODIgaqKD3eOmR0B/DdwCPD91u7YzLLdfWdr9yMikaS6R1pEl6ik1eLDaY8DJlgg28zuM7MlZrbczK4DMLMsM3vQzFaZ2QtmNtfMLolv22hmd5jZa8BIM/uSmf3ZzN4ws6fj8+FgZieb2R/jk7C9ZGa5oZ24iIRKdY/sjRIcSQh330Dw93QE8J/ANnc/BTgFuNbMegEjgJ5ADLgGOK3RbqrdfSgwD/gucHZ8wrWlwM3xeXF+CVzi7icDvwXuTva5iUj6Ut0jzdElKkkkiz9+CRhY9w2JoPk4HxgKPB2/Vv5PM5vf6PVPxh9PBY4HFgXT4XAQ8SHwgQHAK/H12UBlck5FRDKI6h7ZgxIcSQgz6w3sBN4jqGxudPeXGpU5bx+7+biuKMEcMZc1en0MWOXujb99iUgbpbpHmqNLVNJqZtYNmAr8yoPJzV4CvhFv1sXM+sRnhX0N+Gr8eviRwLBmdvk6MMTMPh9//cFm1gdYB3Qzs9Pi69ub2QnJPDcRSV+qe2Rv1IIjByrHzMrYdavmo8BP49seJrje/YYF7blbgK8AzwJfAFYC64G/ANsa79jdt5jZlcDjZtYhvvq77r4+3vT8CzM7hODv935gVeJPT0TSlOoeaRHNJi4pZWad3f0jMzscWAwMcfd/hh2XiESb6p62Ry04kmovmNmhBJ337lIFIyIporqnjVELjoiIiESOOhmLiIhI5CjBERERkchRgiMiIiKRowRHREREIkcJjoiIiESOEhwRERGJnP8PHbIOE2IyyV0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the range of polynomial degrees to test\n",
    "degrees = list(range(1, 6))\n",
    "\n",
    "train_acc = np.zeros(5)\n",
    "cv_acc = np.zeros(5)\n",
    "train_loss = np.zeros(5)\n",
    "cv_loss = np.zeros(5)\n",
    "\n",
    "# iterate through each degree, preprocess X, fit model and K-fold CV, calculate training vs kfold accuracy and neg_log_loss\n",
    "for degree in tqdm(degrees):\n",
    "    pipeline = get_pipeline(preprocessor, poly_degree=degree)\n",
    "    X = pipeline.fit_transform(df_logreg_trials)\n",
    "    logreg = LogisticRegression(penalty=None, solver='saga', max_iter=10000)\n",
    "    logreg.fit(X, y)\n",
    "    \n",
    "    y_pred = logreg.predict(X)\n",
    "    train_acc[degree - 1] = accuracy_score(y, y_pred)\n",
    "    y_prob = logreg.predict_proba(X)\n",
    "    train_loss[degree - 1] = -log_loss(y, y_prob)\n",
    "    \n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    cv_acc[degree - 1] = cross_val_score(logreg, X, y, cv=kfold, scoring='accuracy', n_jobs=-1).mean()\n",
    "    cv_loss[degree - 1] = cross_val_score(logreg, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1).mean()\n",
    "\n",
    "# Plot the results: training versus k-fold CV accuracy (y-axis) as a function of degree (x-axis)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.8))\n",
    "\n",
    "sns.scatterplot(x=degrees, y=train_acc, color=\"r\", ax=ax1)\n",
    "sns.scatterplot(x=degrees, y=cv_acc, color=\"b\", ax=ax1)\n",
    "ax1.set_xlabel(\"Degree\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.legend([\"Training\", \"Cross validation\"])\n",
    "\n",
    "# Plot the results: training versus k-fold CV negative log-loss (y-axis) as a function of degree (x-axis)\n",
    "\n",
    "sns.scatterplot(x=degrees, y=train_loss, color=\"r\", ax=ax2)\n",
    "sns.scatterplot(x=degrees, y=cv_loss, color=\"b\", ax=ax2)\n",
    "ax2.set_xlabel(\"Degree\")\n",
    "ax2.set_ylabel(\"Negative loss\")\n",
    "ax2.legend([\"Training\", \"Cross validation\"])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 [Write] (5pts)\n",
    "Reflect on the above plots you have generated. What are some conclusions you might draw from them?\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "As we increase the degree, the training accuracy and negative log loss tend to increase steadily, but the cross validation accuracy and negative log loss may decrease due to overfitting. Thus, it is better to use degree at most 2.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to use `GridSearchCV`\n",
    "In the following cell, we have provided code that uses `GridSearchCV` to perform a grid search over the best-performing combination of \n",
    "* polynomial degree (`degree` = [1...5]) $\\times$\n",
    "* regularization type (`l1` versus `l2`) $\\times$\n",
    "* regularization strength (`C`=[0.1...5])\n",
    "\n",
    "The code finds the logreg model with the lowest cross-validation log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__C': 0.25, 'classifier__penalty': 'l1', 'polynomial__degree': 3}\n",
      "Best CV negative log loss:  -0.6310132978576055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline for the whole process\n",
    "pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('polynomial', PolynomialFeatures(include_bias=True)),\n",
    "        ('classifier', LogisticRegression(solver='saga', max_iter=10000))\n",
    "    ])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'polynomial__degree': [1, 2, 3, 4, 5],\n",
    "    'classifier__C': [0.1, 0.25, 0.5, 0.75, 1, 2, 5],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best combination of parameters and fit the data\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_log_loss', refit=True)\n",
    "grid_search.fit(df_logreg_trials, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best CV negative log loss: \", grid_search.best_score_)\n",
    "\n",
    "# Save the best model as best_model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 [Code] (10pts)\n",
    "In this question, you will use the results of grid search above. In the cell below, write code that:\n",
    "\n",
    "1) Uses the best-performing model to calculate the SEPs for all trial cells.\n",
    "\n",
    "2) Plots a histogram of the estimated SEPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD5CAYAAAAgGF4oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQMklEQVR4nO3df+xddX3H8ecLCv4A1DK+YFPbVZGpSBTdFzKLMSI60cwADkRmlDhm2aYGNjUSljgSs8Qt/krmhlQlsMUhTmGgIsoQZQ5kfksqPwZO5hBqCf0ibohbpoX3/vieji/99b0tPefe9vN8JDf3nnPPvZ9Xb9tXTz/3nPNNVSFJasde4w4gSRqWxS9JjbH4JakxFr8kNcbil6TGWPyS1JhFfb1xkicD1wNP6sb5QlX9aZIDgUuBFcDdwJuq6qfbe6+DDjqoVqxY0VdUSdojrVmz5oGqmtp8ffo6jj9JgP2q6uEk+wDfBs4C3gg8WFUfSnIOsLiq3r+995qenq6ZmZleckrSnirJmqqa3nx9b1M9NefhbnGf7lbACcDF3fqLgRP7yiBJ2lKvc/xJ9k6yFtgAXFNVNwGHVNV9AN39wX1mkCQ9Xq/FX1WPVNWRwLOAo5McMeprk6xKMpNkZnZ2treMktSaQY7qqar/BL4JHA/cn2QJQHe/YRuvWV1V01U1PTW1xXcTkqSd1FvxJ5lK8ozu8VOAVwN3AlcCp3ebnQ5c0VcGSdKWejucE1gCXJxkb+b+gfl8VX05yY3A55OcAdwDnNJjBknSZnor/qq6BXjJVtb/BDiur3ElSdvnmbuS1BiLX5IaY/FLu6mly5aTZPDb0mXLx/1L1xPU55e7knq0ft29nHrBDYOPe+mZKwcfU7uWe/yS1BiLX5IaY/FLUmMsfklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhpj8UtSYyx+SWqMxS9JjbH4JakxFr8kNaa34k+yLMl1Se5IcnuSs7r15yX5cZK13e31fWWQJG1pUY/vvRF4T1XdnOQAYE2Sa7rnPlZVH+5xbEnSNvRW/FV1H3Bf9/hnSe4AlvY1niRpNIPM8SdZAbwEuKlb9a4ktyS5MMnibbxmVZKZJDOzs7NDxJSkJvRe/En2B74InF1VDwHnA4cCRzL3P4KPbO11VbW6qqaranpqaqrvmJLUjF6LP8k+zJX+Z6vqMoCqur+qHqmqR4FPAUf3mUGS9Hh9HtUT4DPAHVX10Xnrl8zb7CTgtr4ySJK21OdRPccAbwVuTbK2W3cucFqSI4EC7gbO7DGDJGkzfR7V820gW3nqqr7GlCQtzDN3JakxFr8kNcbil6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhpj8UtSYyx+SWqMxS9JjbH4JakxFr8kNcbil6TG9Fb8SZYluS7JHUluT3JWt/7AJNck+UF3v7ivDJKkLfW5x78ReE9VvQD4DeCdSQ4HzgGurarDgGu7ZUnSQHor/qq6r6pu7h7/DLgDWAqcAFzcbXYxcGJfGSRJWxpkjj/JCuAlwE3AIVV1H8z94wAcvI3XrEoyk2RmdnZ2iJiS1ITeiz/J/sAXgbOr6qFRX1dVq6tquqqmp6am+gsoSY3ptfiT7MNc6X+2qi7rVt+fZEn3/BJgQ58ZJEmP1+dRPQE+A9xRVR+d99SVwOnd49OBK/rKIEna0qIe3/sY4K3ArUnWduvOBT4EfD7JGcA9wCk9ZpAkbaa34q+qbwPZxtPH9TWuJGn7PHNXkhpj8UtSYyx+SWqMxS9JjbH4JakxFr8kNcbil6TGWPyS1BiLX5IaY/FLUmMsfklqjMUvSY2x+CWpMRa/JDXG4pekxoxU/EmOGWWd1Jqly5aTZCw3aWeN+oNY/hJ46QjrpKasX3cvp15ww1jGvvTMlWMZV7u/7RZ/kpcBK4GpJH8876mnAXv3GUyS1I+F9vj3Bfbvtjtg3vqHgJP7CiVJ6s92i7+qvgV8K8lFVfWjgTJJkno06hz/k5KsBlbMf01VvaqPUJKk/oxa/H8PfBL4NPBIf3EkSX0btfg3VtX5vSaRJA1i1BO4vpTkD5MsSXLgpluvySRJvRh1j//07v5989YV8JxdG0eS1LeRir+qnt13EEnSMEYq/iRv29r6qvqb7bzmQuC3gA1VdUS37jzgHcBst9m5VXXVjgSWJD0xo071HDXv8ZOB44CbgW0WP3AR8ImtbPOxqvrwqAElSbvWqFM9756/nOTpwN8u8Jrrk6zY+WiSpD7s7GWZ/xs4bCdf+64ktyS5MMninXwPSdJOGvWyzF9KcmV3+wrwfeCKnRjvfOBQ4EjgPuAj2xlzVZKZJDOzs7Pb2kyStINGneOfPye/EfhRVa3b0cGq6v5Nj5N8CvjydrZdDawGmJ6erh0dS5K0dSPt8XcXa7uTuSt0LgZ+sTODJVkyb/Ek4LadeR9J0s4bdarnTcC/AKcAbwJuSrLdyzInuQS4EXheknVJzgD+IsmtSW4BjgX+6AmllyTtsFGnev4EOKqqNgAkmQL+EfjCtl5QVadtZfVndjihJGmXGvWonr02lX7nJzvwWknSBBl1j//qJF8DLumWTwU841aSdkML/czd5wKHVNX7krwReDkQ5ubuPztAPmkkS5ctZ/26e8cdQ9otLLTH/3HgXICqugy4DCDJdPfcG3rMJo1s/bp7OfWCGwYf99IzVw4+pvRELTRPv6Kqbtl8ZVXNMPdjGCVJu5mFiv/J23nuKbsyiCRpGAsV/3eTvGPzld0x+Wv6iSRJ6tNCc/xnA5cneQuPFf00sC9zZ95KknYz2y3+7to6K5McCxzRrf5KVX2j92SSpF6Mej3+64Dres4iSRqAZ99KUmMsfklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYi1+SGmPxS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhpj8UtSY3or/iQXJtmQ5LZ56w5Mck2SH3T3i/saX5K0dX3u8V8EHL/ZunOAa6vqMODablmSNKDeir+qrgce3Gz1CcDF3eOLgRP7Gl+StHVDz/EfUlX3AXT3B29rwySrkswkmZmdnR0soKQF7LWIJGO5LV22fNy/+j3ConEH2JaqWg2sBpienq4xx5G0yaMbOfWCG8Yy9KVnrhzLuHuaoff470+yBKC73zDw+JLUvKGL/0rg9O7x6cAVA48vSc3r83DOS4AbgeclWZfkDOBDwGuS/AB4TbcsSRpQb3P8VXXaNp46rq8xJUkL88xdSWqMxS9JjbH4JakxFr8kNcbil6TGWPyS1BiLX5IaY/FLUmMsfklqzB5f/EuXLfcSspI0z8RelnlXWb/uXi8hK0nz7PF7/JKkx7P4JakxFr8kNcbil6TGWPyS1BiLfw80rkNYPXxV2j3s8Ydztmhch7B6+Kq0e3CPX5IaY/FLUmMsfklqjMUvSY2x+CWpMR7V06e9FpFk3Ckk6XEs/j49utHDKiVNHKd6JKkxFr8kNWYsUz1J7gZ+BjwCbKyq6XHkkKQWjXOO/9iqemCM40tSk5zqkaTGjKv4C/h6kjVJVm1tgySrkswkmZmdnR04niTtucZV/MdU1UuB1wHvTPKKzTeoqtVVNV1V01NTU8MnlKQ91FiKv6rWd/cbgMuBo8eRQ5JaNHjxJ9kvyQGbHgO/Cdw2dA5JatU4juo5BLi8u5TBIuDvqurqMeSQpCYNXvxV9UPgxUOPK0ma4+GcktQYi1+SFrB02XKSjOW2dNnyXf7r8eqckrSA9evuHcuVdqGfq+26xy9JjbH4JakxFr8kNcbil6TGWPyS1BiP6tGu4w+Xl3YLFr92nTH9cHnwB8xLO8KpHklqjMUvSY2x+CWpMRa/JDXG4pekxlj8ktQYD+eUtPvwXJFdwuKXtPsY07kie9p5Ik71SFJjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhozluJPcnyS7ye5K8k548ggSa0avPiT7A38FfA64HDgtCSHD51Dklo1jj3+o4G7quqHVfUL4HPACWPIIUlNGkfxLwXunbe8rlsnSRpAqmrYAZNTgNdW1e91y28Fjq6qd2+23SpgVbf4POD7gwadcxDwwBjGHcUkZ4PJzjfJ2WCy85lt540j369W1dTmK8dxdc51wLJ5y88C1m++UVWtBlYPFWprksxU1fQ4M2zLJGeDyc43ydlgsvOZbedNUr5xTPV8FzgsybOT7Au8GbhyDDkkqUmD7/FX1cYk7wK+BuwNXFhVtw+dQ5JaNZYfxFJVVwFXjWPsHTTWqaYFTHI2mOx8k5wNJjuf2XbexOQb/MtdSdJ4eckGSWqMxc/Cl5BI8vwkNyb53yTvnbBsb0lyS3e7IcmLJyjbCV2utUlmkrx8qGyj5Ju33VFJHkly8qRkS/LKJP/VfXZrk3xgqGyj5JuXcW2S25N8a1KyJXnfvM/ttu739sAJyfb0JF9K8r3uc3v7ELm2UFVN35j7gvnfgecA+wLfAw7fbJuDgaOAPwPeO2HZVgKLu8evA26aoGz789h04ouAOyfps5u33TeY+87p5EnJBrwS+PJQn9dO5HsG8K/A8m754EnJttn2bwC+MSnZgHOBP+8eTwEPAvsO/XvsHv8Il5Coqg1V9V3glxOY7Yaq+mm3+B3mzouYlGwPV/cnHNgPGPILpVEvDfJu4IvAhgnMNi6j5Psd4LKqugfm/o5MULb5TgMuGSTZaNkKOCBJmNsxehDYOFC+/2fxT/YlJHY02xnAV3tN9JiRsiU5KcmdwFeA3x0oG4yQL8lS4CTgkwPmgtF/X1/WTQl8NckLh4kGjJbv14DFSb6ZZE2St01QNgCSPBU4nrl/2IcwSrZPAC9g7qTVW4GzqurRYeI9ZiyHc06YbGXdpBzqNHK2JMcyV/xDzaOPlK2qLgcuT/IK4IPAq/sO1hkl38eB91fVI3M7YIMZJdvNzJ1u/3CS1wP/ABzWd7DOKPkWAb8OHAc8BbgxyXeq6t8mINsmbwD+uaoe7DHPfKNkey2wFngVcChwTZJ/qqqHes72OO7xj3gJiTEZKVuSFwGfBk6oqp9MUrZNqup64NAkB/UdrDNKvmngc0nuBk4G/jrJiZOQraoeqqqHu8dXAftM2Ge3Dri6qn5eVQ8A1wNDHFiwI3/u3sxw0zwwWra3MzdFVlV1F/AfwPMHyveYob9UmLQbc3suPwSezWNfyLxwG9uex7Bf7i6YDVgO3AWsnLTPDXguj325+1Lgx5uWJyHfZttfxHBf7o7y2T1z3md3NHDPJH12zE1XXNtt+1TgNuCIScjWbfd05ubP9xviM9uBz+184Lzu8SHd34mDhsq46db8VE9t4xISSX6/e/6TSZ4JzABPAx5NcjZz39b3+t+zUbIBHwB+hbm9VYCNNcCFoEbM9tvA25L8Evgf4NTq/sRPSL6xGDHbycAfJNnI3Gf35kn67KrqjiRXA7cAjwKfrqrbJiFbt+lJwNer6ud9Z9rBbB8ELkpyK3NTQ++vuf8xDcozdyWpMc7xS1JjLH5JaozFL0mNsfglqTEWvyQ1xuKXpMZY/JLUGItfkhrzfwqY8c02hZ65AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the best CV model to calculate the SEPs for all trial cells\n",
    "\n",
    "y_prob = best_model.predict_proba(df_logreg_trials)\n",
    "\n",
    "# Plot a histogram of the SEPS\n",
    "\n",
    "sns.histplot(x=y_prob[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Beta-binomial updating and empirical Bayes\n",
    "\n",
    "In this second part, we will approach the problem differently. Instead of using covariates to share information across cells, we will assume that all SEPs come from a **shared prior distribution** $q_k \\stackrel{\\textrm{iid}}{\\sim}P(q)$. \n",
    "\n",
    "This part also incorporates the possibility of multiple trials $N_k$ per cell. The outcome data is now\n",
    "\\begin{align}\n",
    "y_k &\\in \\{0,\\dots, N_k\\}&\\textrm{ the number of successful trials}\n",
    "\\end{align}\n",
    "\n",
    "We will assume the following beta-binomial model:\n",
    "\n",
    "\\begin{align}\n",
    "q_k &\\stackrel{\\textrm{iid}}{\\sim} \\textrm{Beta}(a_0,\\,b_0) \\\\\n",
    "y_k &\\stackrel{\\textrm{ind.}}{\\sim} \\textrm{Binom}(N_k,\\, q_k)\n",
    "\\end{align}\n",
    "\n",
    "In this model, we assume the SEPs $q_k$ across all cells $k$ were drawn from a shared beta distribution with shape parameters $a_0$ and $b_0$. The number of successes $y_k$ conditioned on the number of trials $N_k$ and $q_k$ is then binomially-distributed.\n",
    "\n",
    "Put another way, this implies the following **marginal likelihood** for the trial data:\n",
    "\n",
    "\\begin{align}\n",
    "P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{N}_{1:K}, a_0,\\,b_0\\,) &= \\prod_{k=1}^K P(y_k \\mid N_k, a_0,\\,b_0\\,) \\\\\n",
    "&= \\prod_{k=1}^K \\int \\underbrace{P(y_k \\mid N_k,\\, q_k)}_{\\textrm{Binom}(y_k; N_k,\\,q_k)} \\, \\underbrace{P(q_k \\mid a_0,\\, b_0)}_{\\textrm{Beta}(q_k;a_0,\\,b_0)}\\,dq_k\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 [Math] (10pts)\n",
    "1) Provide an analytic expression (i.e., without any integrals) for the negative log marginal likelihood:\n",
    "\\begin{align}\n",
    "-\\log P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{N}_{1:K}, a_0,\\,b_0\\,) &=-\\sum_{k=1}^K\\log\\left({N_k\\choose y_k}\\frac{B(a_0+y_k, b_0+N_k-y_k)}{B(a_0,b_0)}\\right)\\\\\n",
    "&= \\sum_{k=1}^K\\left(\\sum_{i=1}^{y_k}\\log\\left(\\frac{a_0+i-1}{i}\\right) + \\sum_{i=1}^{N_k-y_k}\\log\\left(\\frac{b_0+i-1}{i}\\right) - \\sum_{i=1}^{N_k}\\log\\left(\\frac{a_0+b_0+i-1}{i}\\right)\\right)\n",
    "\\end{align}\n",
    "\n",
    "2) Briefly explain how you came to this expression.\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "Plugging in the densities for binomial and beta distributions, we see that\n",
    "\n",
    "\\begin{align}\n",
    "    P(y_k\\mid N_k,\\,q_k)P(q_k\\mid a_0,\\,b_0)&={N_k\\choose y_k}q_k^{y_k}(1-q_k)^{N_k-q_k}\\cdot\\frac{1}{B(a_0, b_0)}q_k^{a_0-1}(1-q_k)^{b_0-1}\\,.\n",
    "\\end{align}\n",
    "\n",
    "This is again the form of a beta density, so this gives\n",
    "\n",
    "\\begin{align}\n",
    "    \\int P(y_k\\mid N_k,\\,q_k)P(q_k\\mid a_0,\\,b_0)\\,dq_k&={N_k\\choose y_k}\\cdot\\frac{1}{B(a_0,b_0)}\\cdot B(a_0+y_k, b_0+N_k-y_k)\\\\\n",
    "    &=\\frac{N_k!}{y_k!(N_k-y_k)!}\\cdot\\frac{\\Gamma(a_0+b_0)}{\\Gamma(a_0)\\Gamma(b_0)}\\cdot\\frac{\\Gamma(a_0+y_k)\\Gamma(b_0+N_k-y_k)}{\\Gamma(a_0+b_0+N_k)}\\,.\n",
    "\\end{align}\n",
    "\n",
    "Using the relation $\\Gamma(x+1)=x\\Gamma(x)$, we obtain the result.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 [Code] (15pts)\n",
    "\n",
    "In this question, you will learn to use `scipy.optimize.minimize` to minimize a custom function. Make sure to read the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) and to experiment with different settings.\n",
    "\n",
    "Write the following code in the cell below:\n",
    "\n",
    "1) Implement a function to compute the negative log marginal likelihood. Your function can use **any** functions available in the `numpy` or `scipy` Python libraries.\n",
    "\n",
    "\n",
    "2) Implement a function that estimates the parameters $a_0, b_0$ using type-II maximum likelihood by minimizing your function `neg_log_marginal_likelihood` using `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import scipy.special as sc\n",
    "\n",
    "def neg_log_marginal_likelihood(params, y, n):\n",
    "    \"\"\" Calculate the negative log-marginal likelihood for the beta-binomial model.\n",
    "    \n",
    "    Args:\n",
    "        params (tuple): the parameters of the marginal likelihood (a0, b0)\n",
    "        y (array): the number of successes for each trial\n",
    "        n (array): the number of trials\n",
    "    \n",
    "    Returns:\n",
    "        float: the negative log-marginal likelihood\n",
    "    \"\"\"\n",
    "    a0, b0 = params\n",
    "    \n",
    "    a_pos = np.array(y) + a0\n",
    "    b_pos = np.array(n) - y + b0\n",
    "\n",
    "    return np.sum(np.log(sc.beta(a0, b0)) - np.log(sc.binom(n, y)) - np.log(sc.beta(a_pos, b_pos)))\n",
    "\n",
    "def fit_marginal_likelihood(y, n):\n",
    "    \"\"\" Fit the parameters of the marginal likelihood to the data using maximum likelihood.\n",
    "\n",
    "    Uses scipy.optimize.minimize and calls neg_log_marginal_likelihood.\n",
    "\n",
    "    Args:\n",
    "        y (array): the number of successes for each trial\n",
    "        n (array): the number of trials\n",
    "        \n",
    "        If you add other args, describe them here.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: the MLE for a0 and b0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    res = minimize(neg_log_marginal_likelihood, (1, 1), args=(y, n), bounds=((0, None), (0, None)))\n",
    "    a0_mle, b0_mle = res.x\n",
    "\n",
    "    return a0_mle, b0_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your code on the trial data\n",
    "\n",
    "Run the following code, which:\n",
    "1) Loads the trial data. Note that this time, we have multiple trials per cell $N_k$ (`n_trials`) and potentially multiple successes per cell $y_k$ (`n_successes`)\n",
    "\n",
    "2) Runs your `fit_marginal_likelihood` function on the data. Make sure to modify this line if your function takes additional arguments\n",
    "\n",
    "3) Plots the estimated **empirical prior**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.785774965311204 4.6046266226299055\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load in the beta-binomial trial data\n",
    "df_trials = pd.read_csv('betabin_trial_cells.csv')\n",
    "y = df_trials['n_successes'].values\n",
    "n = df_trials['n_trials'].values\n",
    "\n",
    "# if your fit_marginal_likelihood function takes extra args, modify this code to pass them in here.\n",
    "a0_mle, b0_mle = fit_marginal_likelihood(y, n)\n",
    "print(a0_mle, b0_mle)\n",
    "\n",
    "# plot the empirical prior beta distribution\n",
    "x_axis_vals = np.linspace(0, 1, 100)\n",
    "y_axis_vals = st.beta.pdf(x_axis_vals, a0_mle, b0_mle)\n",
    "_ = plt.plot(x_axis_vals, y_axis_vals, color='Green')\n",
    "_ = plt.fill_between(x_axis_vals, y_axis_vals, alpha=0.2, color='Green')\n",
    "_ = plt.title('Empirical Prior Beta Distribution')\n",
    "_ = plt.xlabel('SEP')\n",
    "_ = plt.ylabel('density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 [Math] (10pts)\n",
    "\n",
    "1) Provide the form of the posterior over cell $k$'s SEP $q_k$, conditioned on the number of trials $N_k$ and successes $y_k$ in the cell, and the estimated parameters $\\hat{a_0}$, $\\hat{b_0}$:\n",
    "\\begin{equation}\n",
    "P(q_k \\mid y_k,\\, N_k,\\,\\hat{a_0}, \\hat{b_0}) = \\frac{1}{B(\\hat{a}_0+y_k,\\hat{b}_0+N_k-y_k)}q_k^{\\hat{a}_0+y_k}(1-q_k)^{\\hat{b}_0+N_k-y_k}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "2) Provide the form of the posterior expectation\n",
    "\\begin{equation}\n",
    "E[q_k \\mid y_k,\\, N_k,\\,\\hat{a_0}, \\hat{b_0}] = \\frac{\\hat{a}_0+y_k}{\\hat{a}_0+\\hat{b}_0+N_k}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 [Code] (5pts)\n",
    "1) Implement a function to compute the posterior expectation above.\n",
    "\n",
    "2) Compute the posterior expectation of $q_k$ for all trial cells.\n",
    "\n",
    "3) Plot a histogram of the estimated SEPs. (Run the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def posterior_expectation(y, n, a0, b0):\n",
    "    \"\"\" Calculate the posterior expectation of SEP.\n",
    "\n",
    "    Args:\n",
    "        y (array): the number of successes for each trial\n",
    "        n (array): the number of trials\n",
    "        a0 (float): the prior alpha parameter\n",
    "        b0 (float): the prior beta parameter\n",
    "    \n",
    "    Returns:\n",
    "        array: the posterior expectation of SEP\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    return (np.array(y) + a0) / (np.array(n) + a0 + b0)\n",
    "\n",
    "estimated_SEP = posterior_expectation(y, n, a0_mle, b0_mle)\n",
    "_ = plt.hist(estimated_SEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question [Write] (10pts) \n",
    "\n",
    "1) Reflect on the pros and cons of the two approaches we used to estimate SEPs (supervised learning and posterior updating). Under what conditions might we prefer one over the other?\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "The first approach leverages other information we have about the cell, but since we send divers to each cell only once, we should have many cells to obtain a reliable estimate. On the other hand, the second approach does not utilize the cell information and assumes a priori that every cell is independent and identical, but instead sends divers many times to each cell, so that we have a more information about the success probability of each cell.\n",
    "\n",
    "If the number of cells is large and there is a limitation on the number of divers we can send, then the first approach may be more preferable. If there are only a few cells so that we can send many divers to each cell, or if we don't have a specific information about the cells but have a prior belief that all the cells are similar, then we may choose to do the second method.\n",
    "\n",
    "___\n",
    "\n",
    "2) Reflect on how we might combine these approaches. Describe a third approach that would combine elements of both:\n",
    "\n",
    "___\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "In the empirical Bayesian approach, we set the prior under the assumption that all the SEPs are independent and identically distributed. However, if we have a prior information about the cells, then we might want to set a different prior where we take the cell features into account. For this, we can utilize such information using supervised learning, which may lead to a better prior.\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOEIcgdHfyD4yGF2Q4D7WMS",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e31d8529818713d98a38061e476a303cb7aebfe46db2981fe05de46bbfa528ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
